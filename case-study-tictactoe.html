<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fallstudie: Selbstlernendes Tic-Tac-Toe - Jens Fischer</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700;900&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="styles.css">
</head>
<body class="bg-gray-100 text-gray-800">

    <div class="bg-white sticky top-0 z-50 border-b border-gray-200">
        <!-- Header -->
        <div class="container max-w-6xl mx-auto px-6">
            <div class="flex justify-between items-center py-3">
                <a href="index.html" class="text-2xl font-bold text-gray-800">Jens Fischer</a>
            </div>
            <!-- Navigation -->
            <nav class="border-t border-gray-200">
                <div class="flex justify-between items-center py-2">
                    <div class="hidden md:flex space-x-8">
                        <a href="index.html" class="nav-link text-gray-600 hover:text-teal-600 transition duration-300">Startseite</a>
                        <a href="werdegang.html" class="nav-link text-gray-600 hover:text-teal-600 transition duration-300">Mein Werdegang</a>
                        <a href="case-studies.html" class="nav-link active">Fallstudien</a>
                    </div>
                </div>
            </nav>
        </div>
    </div>

    <main class="container max-w-6xl mx-auto p-6 md:p-12">
        <section class="bg-white p-8 md:p-16 rounded-2xl shadow-lg">
            <div class="prose prose-lg max-w-none">
                 <a href="case-studies.html" class="text-teal-600 font-semibold hover:underline">&larr; Zurück zu den Fallstudien</a>
                <h1 class="text-5xl font-black text-gray-900 mt-4">Fallstudie: Selbstlernendes Tic-Tac-Toe auf dem Atari</h1>
                <p class="text-xl text-gray-500 lead">Ein frühes Experiment aus den 1980er Jahren, das die Kernprinzipien des modernen Reinforcement Learning intuitiv vorwegnahm.</p>

                <h2 class="mt-10 text-3xl font-bold">Die Herausforderung</h2>
                <p>Auf einem Atari 400XL, einem Computer mit extrem begrenzten Ressourcen, sollte ein Tic-Tac-Toe-Programm entstehen, das nicht nur nach festen Regeln spielt, sondern aus seinen eigenen Spielen lernt und mit der Zeit immer besser wird.</p>

                <h2 class="mt-10 text-3xl font-bold">Die Umsetzung: Intuitives Reinforcement Learning</h2>
                <p>Der Ansatz war eine einfache, aber effektive Form des Lernens durch Erfahrung:</p>
                <ol class="list-decimal list-inside space-y-4 mt-6">
                    <li><strong>Regelwerk als Basis:</strong> Zuerst wurde das grundlegende Regelwerk von Tic-Tac-Toe implementiert. Das Programm wusste, welche Züge legal sind und wann ein Spiel gewonnen, verloren oder unentschieden ist.</li>
                    <li><strong>Aufbau einer Datenbasis:</strong> Jeder mögliche Spielzug in jeder denkbaren Spielsituation wurde in einer Datenbasis gespeichert.</li>
                    <li><strong>Lernen durch Gewichtung:</strong> Das war der entscheidende Schritt. Nach jedem abgeschlossenen Spiel wurden alle Züge, die zu diesem Ergebnis geführt haben, bewertet:
                        <ul class="list-disc pl-5 mt-2 space-y-2">
                            <li>Züge, die zu einem <strong>Sieg</strong> führten, erhielten einen positiven Wert (ihr "Gewicht" wurde erhöht).</li>
                            <li>Züge, die zu einer <strong>Niederlage</strong> führten, erhielten einen negativen Wert (ihr "Gewicht" wurde gesenkt).</li>
                        </ul>
                    </li>
                    <li><strong>Die Strategie:</strong> Bei seinem nächsten Zug wählte das Programm immer den legalen Zug mit dem höchsten erlernten Gewicht aus.</li>
                </ol>

                <h2 class="mt-16 text-3xl font-bold">Die Verbindung zur modernen KI</h2>
                <p>Dieses einfache System ist im Grunde eine Vorform dessen, was DeepMind Jahrzehnte später mit **AlphaGo** und **AlphaZero** perfektionierte. Es demonstriert die Kernidee des **Reinforcement Learning durch Selbstspiel**:</p>
                <ul class="list-disc pl-5 mt-4 space-y-2">
                    <li>Das System benötigt kein menschliches Expertenwissen, sondern lernt ausschließlich aus den Ergebnissen seiner eigenen Aktionen.</li>
                    <li>Es entwickelt eine "Value Function", also eine Bewertung für die Güte eines Zuges oder einer Spielsituation.</li>
                    <li>Durch Tausende von Spielen (bei AlphaZero Milliarden) wird diese Bewertungsfunktion immer genauer und führt zu einer übermenschlichen Spielstärke.</li>
                </ul>

                <h2 class="mt-16 text-3xl font-bold">Fazit</h2>
                <p>Dieses frühe Projekt zeigt, dass die fundamentalen Ideen der KI oft älter sind als die Technologie, um sie in großem Stil umzusetzen. Es unterstreicht, wie wichtig praktische Erfahrung und ein intuitives Verständnis für Lernprozesse sind, um die komplexen Systeme von heute zu verstehen und zu gestalten.</p>
            </div>
        </section>
    </main>
    
    <footer class="bg-white mt-16">
        <div class="container max-w-6xl mx-auto px-6 py-8 text-center text-gray-600">
            <a href="sitemap.html" class="text-blue-600 hover:underline">Sitemap</a>
            <p class="mt-4">&copy; 2025 Jens Fischer. Alle Rechte vorbehalten.</p>
        </div>
    </footer>

</body>
</html>
