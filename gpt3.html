<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GPT-3: Die Macht der Skalierung - Innovate Solutions</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .nav-link.active {
            color: #2563EB; /* blue-600 */
            font-weight: 600;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <!-- Header und Navigation -->
    <header class="bg-white shadow-md sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="index.html" class="text-2xl font-bold text-blue-600">Innovate Solutions</a>
            <div class="hidden md:flex space-x-6">
                <a href="index.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Startseite</a>
                <a href="about.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Über uns</a>
                <a href="services.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Dienstleistungen</a>
                <a href="history.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">KI-Geschichte</a>
                <a href="contact.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Kontakt</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>
        </nav>
        <!-- Mobiles Menü -->
        <div id="mobile-menu" class="hidden md:hidden">
            <a href="index.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Startseite</a>
            <a href="about.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Über uns</a>
            <a href="services.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Dienstleistungen</a>
            <a href="history.html" class="block py-2 px-4 text-sm hover:bg-gray-100">KI-Geschichte</a>
            <a href="contact.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Kontakt</a>
        </div>
    </header>

    <main class="container mx-auto p-6 md:p-12">
        <!-- INHALT DER "GPT-3"-SEITE -->
        <section class="bg-white p-8 md:p-12 rounded-xl shadow-lg">
            <h1 class="text-4xl font-bold text-gray-900 mb-4">GPT-3: Language Models are Few-Shot Learners (2020)</h1>
            <p class="text-lg text-gray-500 mb-8">Wie pure Skalierung ein neues Paradigma der KI einläutete.</p>

            <div class="prose prose-lg max-w-none">
                <p>Das 2020 von OpenAI veröffentlichte Paper zu GPT-3 (Generative Pre-trained Transformer 3) war ein Meilenstein, der die KI-Welt erschütterte. Die zentrale These war einfach, aber ihre Auswirkungen waren gewaltig: Wenn man ein Transformer-Modell auf eine bis dahin unvorstellbare Größe skaliert, entwickelt es neue, "emergente" Fähigkeiten, die über das hinausgehen, was kleinere Modelle leisten können. GPT-3, mit seinen 175 Milliarden Parametern, war der Beweis für diese These.</p>

                <h2 class="mt-10">Die Revolution: In-Context Learning</h2>
                <p>Vor GPT-3 war der gängige Ansatz, ein vorab trainiertes Modell (wie BERT) für eine spezifische Aufgabe durch ein aufwändiges Fine-Tuning auf einem gelabelten Datensatz anzupassen. GPT-3 machte diesen Schritt für viele Aufgaben überflüssig und führte stattdessen das Konzept des <strong>In-Context Learning</strong> ein. Das Modell kann eine Aufgabe lösen, indem es einfach eine Beschreibung der Aufgabe und einige wenige Beispiele im Eingabe-Prompt erhält – ganz ohne Anpassung der Modellgewichte.</p>
                <p>Das Paper popularisierte drei Arten des In-Context Learning:</p>
                <ol>
                    <li><strong>Zero-Shot:</strong> Das Modell erhält nur die Aufgabenbeschreibung. (z.B. "Übersetze 'Käse' ins Englische.")</li>
                    <li><strong>One-Shot:</strong> Das Modell erhält ein einziges Beispiel. (z.B. "Meer -> sea, Käse ->")</li>
                    <li><strong>Few-Shot:</strong> Das Modell erhält mehrere Beispiele, um den Kontext und das gewünschte Ausgabeformat besser zu verstehen.</li>
                </ol>
                <p>Die Fähigkeit, auf diese Weise zu "lernen", war eine direkte Folge der massiven Skalierung. GPT-3 hatte während seines Trainings auf einem riesigen Datensatz (dem Common Crawl) so viele Muster und Zusammenhänge gelernt, dass es neue Aufgaben durch reines Textverständnis generalisieren konnte.</p>

                <h2 class="mt-10">Vom Fine-Tuning zum Prompt Engineering</h2>
                <p>Die Veröffentlichung von GPT-3 löste eine seismische Verschiebung in der KI-Entwicklung aus. Der Fokus verlagerte sich von der datenintensiven Aufgabe des Fine-Tunings hin zur Kunst und Wissenschaft des <strong>Prompt Engineering</strong>. Die zentrale Frage war nicht mehr "Wie trainiere ich ein Modell für diese Aufgabe?", sondern "Wie formuliere ich meine Anfrage (den Prompt) so, dass das Modell die Aufgabe korrekt löst?".</p>
                <p>Dies machte die Leistungsfähigkeit großer Sprachmodelle plötzlich für eine viel breitere Gruppe von Entwicklern und Nutzern zugänglich, die nicht über die Ressourcen für ein aufwändiges Modelltraining verfügten.</p>

                <div class="bg-gray-100 p-6 rounded-lg my-8 text-center">
                    <svg width="100%" height="200" viewBox="0 0 500 150" xmlns="http://www.w3.org/2000/svg" font-family="Inter, sans-serif" font-size="12px">
                         <defs>
                            <marker id="arrowhead-gpt3" markerWidth="10" markerHeight="7" refX="5" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4B5563"></polygon>
                            </marker>
                        </defs>
                        <!-- Prompt -->
                        <rect x="20" y="50" width="180" height="50" rx="5" fill="#F3F4F6" stroke="#D1D5DB" stroke-width="2"/>
                        <text x="30" y="70" font-weight="bold">Prompt:</text>
                        <text x="30" y="85">"Übersetze: Meer -> sea, ..."</text>

                        <!-- GPT-3 Model -->
                        <rect x="250" y="40" width="100" height="70" rx="10" fill="#DBEAFE" stroke="#60A5FA" stroke-width="2"/>
                        <text x="280" y="75" font-weight="bold">GPT-3</text>
                        <text x="260" y="95">(175B Parameter)</text>

                        <!-- Output -->
                        <text x="400" y="80" font-weight="bold">Antwort</text>

                        <!-- Arrows -->
                        <line x1="200" y1="75" x2="250" y2="75" stroke="#4B5563" stroke-width="2" marker-end="url(#arrowhead-gpt3)"/>
                        <line x1="350" y1="75" x2="400" y2="75" stroke="#4B5563" stroke-width="2" marker-end="url(#arrowhead-gpt3)"/>
                    </svg>
                    <p class="text-sm text-gray-600 mt-2">Das Paradigma des In-Context Learning: Ein Prompt mit Beispielen wird an das Modell gesendet, das eine Antwort generiert, ohne seine Gewichte zu ändern.</p>
                </div>

                <h2 class="mt-10">Auswirkungen und Vermächtnis</h2>
                <p>GPT-3 hat die Landschaft der KI für immer verändert. Es bewies, dass quantitative Steigerungen (mehr Daten, mehr Parameter, mehr Rechenleistung) zu qualitativen Sprüngen in den Fähigkeiten führen können. Es war der direkte Vorläufer von noch leistungsfähigeren Modellen wie InstructGPT und GPT-4 und legte den Grundstein für den globalen Erfolg von Anwendungen wie ChatGPT.</p>
                <p>Gleichzeitig machte das Paper auch auf die Risiken aufmerksam: Das Modell konnte schädliche, voreingenommene oder unwahre Texte generieren, was die dringende Notwendigkeit der Forschung zum Thema "AI Alignment" (die Ausrichtung von KI-Verhalten an menschlichen Werten) unterstrich – eine Herausforderung, die die KI-Forschung bis heute beschäftigt.</p>
            </div>
        </section>
    </main>
    
    <!-- Footer -->
    <footer class="bg-white mt-16">
        <div class="container mx-auto px-6 py-8 text-center text-gray-600">
            <p>&copy; 2025 Innovate Solutions. Alle Rechte vorbehalten.</p>
        </div>
    </footer>

    <script>
        // Skript für das mobile Menü
        document.getElementById('mobile-menu-button').addEventListener('click', function() {
            document.getElementById('mobile-menu').classList.toggle('hidden');
        });
    </script>
</body>
</html>
