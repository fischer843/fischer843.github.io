<!DOCTYPE html>
<html lang="de">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ResNet: Deep Residual Learning - Innovate Solutions</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .nav-link.active {
            color: #2563EB; /* blue-600 */
            font-weight: 600;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <!-- Header und Navigation -->
    <header class="bg-white shadow-md sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <a href="index.html" class="text-2xl font-bold text-blue-600">Innovate Solutions</a>
            <div class="hidden md:flex space-x-6">
                <a href="index.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Startseite</a>
                <a href="about.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Über uns</a>
                <a href="services.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Dienstleistungen</a>
                <a href="history.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">KI-Geschichte</a>
                <a href="contact.html" class="nav-link text-gray-600 hover:text-blue-600 transition duration-300">Kontakt</a>
            </div>
            <button id="mobile-menu-button" class="md:hidden focus:outline-none">
                <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
            </button>
        </nav>
        <!-- Mobiles Menü -->
        <div id="mobile-menu" class="hidden md:hidden">
            <a href="index.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Startseite</a>
            <a href="about.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Über uns</a>
            <a href="services.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Dienstleistungen</a>
            <a href="history.html" class="block py-2 px-4 text-sm hover:bg-gray-100">KI-Geschichte</a>
            <a href="contact.html" class="block py-2 px-4 text-sm hover:bg-gray-100">Kontakt</a>
        </div>
    </header>

    <main class="container mx-auto p-6 md:p-12">
        <!-- INHALT DER "ResNet"-SEITE -->
        <section class="bg-white p-8 md:p-12 rounded-xl shadow-lg">
            <h1 class="text-4xl font-bold text-gray-900 mb-4">ResNet: Deep Residual Learning (2015)</h1>
            <p class="text-lg text-gray-500 mb-8">Die Überwindung der Tiefenbarriere in neuronalen Netzen.</p>

            <div class="prose prose-lg max-w-none">
                <p>Im Jahr 2015 präsentierten Kaiming He und sein Team bei Microsoft Research eine Arbeit, die eines der hartnäckigsten Probleme im Bereich der Computer Vision löste: das Training extrem tiefer neuronaler Netze. Das Paper "Deep Residual Learning for Image Recognition" führte die ResNet-Architektur ein und ermöglichte einen Leistungssprung, der das Feld nachhaltig veränderte.</p>

                <h2 class="mt-10">Das Problem: Die Degradations-Barriere</h2>
                <p>Vor ResNet herrschte die Annahme, dass tiefere Netze grundsätzlich leistungsfähiger sein müssten. In der Praxis zeigte sich jedoch ein paradoxes Phänomen: Fügte man einem bereits tiefen Netzwerk weitere Schichten hinzu, stieg nicht nur der Testfehler (Overfitting), sondern auch der Trainingsfehler selbst an. Dieses als <strong>Degradationsproblem</strong> bekannte Phänomen deutete darauf hin, dass die Optimierungsalgorithmen Schwierigkeiten hatten, die immer komplexer werdenden Netzwerke effektiv zu trainieren.</p>

                <h2 class="mt-10">Die Kernidee: Residual Learning</h2>
                <p>Die Lösung von ResNet war ebenso elegant wie wirkungsvoll. Anstatt zu versuchen, dass ein Stapel von Schichten eine gewünschte Funktion $H(x)$ direkt lernt, formulierten die Autoren das Problem um. Sie ließen die Schichten eine <strong>Residualfunktion</strong> $F(x) := H(x) - x$ lernen. Die ursprüngliche Funktion wird dann einfach durch $F(x) + x$ wiederhergestellt.</p>
                <p>Implementiert wird dies durch sogenannte <strong>"Shortcut Connections"</strong> (oder Skip Connections). Diese leiten die Eingabe $x$ einer Schicht um den Block herum und addieren sie direkt zur Ausgabe des Blocks. Dieser simple Trick hat zwei entscheidende Vorteile:</p>
                <ol>
                    <li><strong>Einfacheres Lernen:</strong> Wenn die optimale Funktion für einen Block eine einfache Identitätsabbildung wäre (also die Eingabe unverändert durchzureichen), müssen die Schichten im Block nur lernen, eine Null-Funktion ($F(x)=0$) auszugeben, was für den Optimierer trivial ist.</li>
                    <li><strong>Besserer Gradientenfluss:</strong> Die Shortcuts schaffen einen direkten Pfad für den Gradienten während der Backpropagation. Dies verhindert das Problem des "verschwindenden Gradienten" und ermöglicht ein effektives Training von Netzen mit Hunderten oder sogar Tausenden von Schichten.</li>
                </ol>

                <div class="bg-gray-100 p-6 rounded-lg my-8 text-center">
                    <svg width="100%" height="250" viewBox="0 0 400 200" xmlns="http://www.w3.org/2000/svg" font-family="Inter, sans-serif" font-size="12px">
                         <defs>
                            <marker id="arrowhead-resnet" markerWidth="10" markerHeight="7" refX="5" refY="3.5" orient="auto">
                                <polygon points="0 0, 10 3.5, 0 7" fill="#4B5563"></polygon>
                            </marker>
                        </defs>
                        <!-- Input -->
                        <text x="10" y="105" font-weight="bold">x</text>
                        <line x1="30" y1="100" x2="80" y2="100" stroke="#4B5563" stroke-width="2" marker-end="url(#arrowhead-resnet)"/>

                        <!-- Residual Block -->
                        <rect x="80" y="60" width="120" height="80" rx="5" fill="#F3F4F6" stroke="#D1D5DB" stroke-width="2"/>
                        <text x="100" y="95">Gewichtete</text>
                        <text x="110" y="110">Schicht + ReLU</text>
                        <line x1="200" y1="100" x2="250" y2="100" stroke="#4B5563" stroke-width="2" marker-end="url(#arrowhead-resnet)"/>

                        <!-- Addition -->
                        <circle cx="270" cy="100" r="15" fill="#D1FAE5" stroke="#34D399" stroke-width="2"/>
                        <text x="265" y="105" font-size="16px" font-weight="bold">+</text>
                        <line x1="285" y1="100" x2="335" y2="100" stroke="#4B5563" stroke-width="2" marker-end="url(#arrowhead-resnet)"/>
                        <text x="340" y="105" font-weight="bold">H(x)</text>

                        <!-- Shortcut Connection -->
                        <path d="M 50 100 Q 50 20 170 20 Q 270 20 270 85" stroke="#3B82F6" stroke-width="2" fill="none" stroke-dasharray="4,4" marker-end="url(#arrowhead-resnet)"/>
                        <text x="150" y="45" fill="#3B82F6">Shortcut</text>
                    </svg>
                    <p class="text-sm text-gray-600 mt-2">Ein Residual Block: Die Eingabe x wird sowohl durch die Schichten geleitet als auch über eine "Shortcut Connection" direkt zur Ausgabe addiert.</p>
                </div>

                <h2 class="mt-10">Auswirkungen und Vermächtnis</h2>
                <p>Die Auswirkungen von ResNet waren sofort und tiefgreifend. Das Modell gewann den renommierten ImageNet Large Scale Visual Recognition Challenge (ILSVRC) im Jahr 2015 mit einem Top-5-Fehler von nur 3,57%, was die menschliche Leistungsfähigkeit übertraf. Es reduzierte den Fehler im Vergleich zum Vorjahressieger um fast die Hälfte.</p>
                <p>ResNet wurde schnell zur De-facto-Standardarchitektur für eine Vielzahl von Computer-Vision-Aufgaben und seine Kernidee der Residual Connections wurde zu einem fundamentalen Baustein in vielen nachfolgenden Architekturen, einschließlich der Transformer-Modelle. Die Arbeit bewies, dass Tiefe ein entscheidender Faktor für die Leistungsfähigkeit von neuronalen Netzen ist, und lieferte das entscheidende Werkzeug, um diese Tiefe auch in der Praxis nutzbar zu machen.</p>
            </div>
        </section>
    </main>
    
    <!-- Footer -->
    <footer class="bg-white mt-16">
        <div class="container mx-auto px-6 py-8 text-center text-gray-600">
            <p>&copy; 2025 Innovate Solutions. Alle Rechte vorbehalten.</p>
        </div>
    </footer>

    <script>
        // Skript für das mobile Menü
        document.getElementById('mobile-menu-button').addEventListener('click', function() {
            document.getElementById('mobile-menu').classList.toggle('hidden');
        });
    </script>
</body>
</html>
